{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55eb47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchdiffeq import odeint_adjoint as odeint\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from typing import Tuple, Any\n",
    "\n",
    "\n",
    "# import os\n",
    "\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import interflow as itf\n",
    "import interflow.prior as prior\n",
    "import interflow.fabrics\n",
    "import interflow.stochastic_interpolant as stochastic_interpolant\n",
    "from torch import autograd\n",
    "from functorch import jacfwd, vmap\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('CUDA available, setting default tensor residence to GPU.')\n",
    "    itf.util.set_torch_device('cuda')\n",
    "else:\n",
    "    print('No CUDA device found!')\n",
    "print(itf.util.get_torch_device())\n",
    "\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f48119",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3330e609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab(var):\n",
    "    \"\"\"Take a tensor off the gpu and convert it to a numpy array on the CPU.\"\"\"\n",
    "    return var.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "def _compute_mu(i):\n",
    "    \"\"\"Compute the ith mean for a GMM.\"\"\"\n",
    "    return 10.0 * torch.Tensor([[\n",
    "        torch.tensor(i * math.pi / 4).sin(),\n",
    "        torch.tensor(i * math.pi / 4).cos()\n",
    "    ]])\n",
    "\n",
    "\n",
    "def compute_likelihoods(\n",
    "    v: torch.nn.Module,\n",
    "    s: torch.nn.Module,\n",
    "    interpolant: stochastic_interpolant.Interpolant,\n",
    "    n_save: int,\n",
    "    n_likelihood: int,\n",
    "    eps: int,\n",
    "    bs: int\n",
    ") -> Tuple[torch.tensor, torch.tensor]:\n",
    "    \"\"\"Draw samples from the probability flow and SDE models, and compute likelihoods.\"\"\"\n",
    "    sde_flow = stochastic_interpolant.SDEIntegrator(\n",
    "        v=v, s=s, dt=torch.tensor(1e-2), eps=eps, interpolant=interpolant, n_save=n_save, n_likelihood=n_likelihood\n",
    "    )\n",
    "\n",
    "    pflow = stochastic_interpolant.PFlowIntegrator(v=v, s=s,  \n",
    "                                                  method='dopri5', \n",
    "                                                  interpolant=interpolant,\n",
    "                                                  n_step=3)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        x0_tests  = base(bs)\n",
    "        xfs_sde   = sde_flow.rollout_forward(x0_tests) # [n_save x bs x dim]\n",
    "        xf_sde    = grab(xfs_sde[-1].squeeze())        # [bs x dim]\n",
    "        \n",
    "        # ([n_likelihood, bs, dim], [bs])\n",
    "        x0s_sdeflow, dlogps_sdeflow = sde_flow.rollout_likelihood(xfs_sde[-1])\n",
    "        log_p0s = torch.reshape(\n",
    "            base.log_prob(x0s_sdeflow.reshape((n_likelihood*bs, ndim))),\n",
    "            (n_likelihood, bs)\n",
    "        )\n",
    "        logpx_sdeflow = torch.mean(log_p0s, axis=0) - dlogps_sdeflow\n",
    "\n",
    "\n",
    "    logp0                  = base.log_prob(x0_tests)            # [bs]\n",
    "    xfs_pflow, dlogp_pflow = pflow.rollout(x0_tests)            # [n_save x bs x dim], [n_save x bs]\n",
    "    logpx_pflow            = logp0 + dlogp_pflow[-1].squeeze()  # [bs]\n",
    "    xf_pflow               = grab(xfs_pflow[-1].squeeze())      # [bs x dim]\n",
    "\n",
    "\n",
    "    return xf_sde, logpx_sdeflow, xf_pflow, logpx_pflow\n",
    "\n",
    "\n",
    "def log_metrics(\n",
    "    v: torch.nn.Module,\n",
    "    s: torch.nn.Module,\n",
    "    interpolant: stochastic_interpolant.Interpolant,\n",
    "    n_save: int,\n",
    "    n_likelihood: int,\n",
    "    likelihood_bs: int, \n",
    "    v_loss: torch.tensor,\n",
    "    s_loss: torch.tensor,\n",
    "    loss: torch.tensor,\n",
    "    v_grad: torch.tensor,\n",
    "    s_grad: torch.tensor,\n",
    "    eps: torch.tensor,\n",
    "    data_dict: dict\n",
    ") -> None:\n",
    "    # log loss and gradient data\n",
    "    v_loss   = grab(v_loss).mean(); data_dict['v_losses'].append(v_loss)\n",
    "    s_loss   = grab(s_loss).mean(); data_dict['s_losses'].append(s_loss)\n",
    "    loss     = grab(loss).mean(); data_dict['losses'].append(loss)\n",
    "    v_grad   = grab(v_grad).mean(); data_dict['v_grads'].append(v_grad)\n",
    "    s_grad   = grab(s_grad).mean(); data_dict['s_grads'].append(s_grad)\n",
    "\n",
    "    \n",
    "    # compute and log likelihood data\n",
    "    _, logpx_sdeflow, _, logpx_pflow = compute_likelihoods(\n",
    "        v, s, interpolant, n_save, n_likelihood, eps, likelihood_bs)\n",
    "    \n",
    "    logpx_sdeflow = grab(logpx_sdeflow).mean(); data_dict['logps_sdeflow'].append(logpx_sdeflow)\n",
    "    logpx_pflow = grab(logpx_pflow).mean(); data_dict['logps_pflow'].append(logpx_pflow)\n",
    "    \n",
    "    \n",
    "def make_plots(\n",
    "    v: torch.nn.Module,\n",
    "    s: torch.nn.Module,\n",
    "    interpolant: stochastic_interpolant.Interpolant,\n",
    "    n_save: int,\n",
    "    n_likelihood: int,\n",
    "    likelihood_bs: int,\n",
    "    counter: int,\n",
    "    metrics_freq: int,\n",
    "    eps: torch.tensor,\n",
    "    data_dict: dict\n",
    ") -> None:\n",
    "    \"\"\"Make plots to visualize samples and evolution of the likelihood.\"\"\"\n",
    "    # compute likelihood and samples for SDE and probability flow.\n",
    "    xf_sde, logpx_sdeflow, xf_pflow, logpx_pflow = compute_likelihoods(\n",
    "        v, s, interpolant, n_save, n_likelihood, eps, likelihood_bs\n",
    "    )\n",
    "\n",
    "\n",
    "    ### plot the loss, test logp, and samples from interpolant flow\n",
    "    fig, axes = plt.subplots(1,4, figsize=(16,4))\n",
    "    print(\"EPOCH:\", counter)\n",
    "    print(\"LOSS, GRAD:\", loss, v_grad, s_grad)\n",
    "\n",
    "\n",
    "    # plot loss over time.\n",
    "    nsaves = len(data_dict['losses'])\n",
    "    epochs = np.arange(nsaves)*metrics_freq\n",
    "    axes[0].plot(epochs, data_dict['losses'], label=\" v + s\")\n",
    "    axes[0].plot(epochs, data_dict['v_losses'], label=\"v\")\n",
    "    axes[0].plot(epochs, data_dict['s_losses'], label = \"s\" )\n",
    "    axes[0].set_title(\"LOSS\")\n",
    "    axes[0].legend()\n",
    "\n",
    "\n",
    "    # plot samples from SDE.\n",
    "    axes[1].scatter(\n",
    "        xf_sde[:,0], xf_sde[:,1], vmin=0.0, vmax=0.05, alpha = 0.2, c=grab(torch.exp(logpx_sdeflow).detach()))\n",
    "    axes[1].set_xlim(-5,5)\n",
    "    axes[1].set_ylim(-6.5,6.5)\n",
    "    axes[1].set_title(\"Samples from SDE\", fontsize=14)\n",
    "\n",
    "\n",
    "    # plot samples from pflow\n",
    "    axes[2].scatter(\n",
    "        xf_pflow[:,0], xf_pflow[:,1], vmin=0.0, vmax=0.05, alpha = 0.2, c=grab(torch.exp(logpx_pflow).detach()))\n",
    "    axes[2].set_xlim(-5,5)\n",
    "    axes[2].set_ylim(-6.5,6.5)\n",
    "    axes[2].set_title(\"Samples from PFlow\", fontsize=14)\n",
    "\n",
    "\n",
    "    # plot likelihood estimates.\n",
    "    axes[3].plot(epochs, data_dict['logps_pflow'],   label='pflow', color='purple')\n",
    "    axes[3].plot(epochs, data_dict['logps_sdeflow'], label='sde',   color='red')\n",
    "    # axes[3].hlines(\n",
    "    #     y=grab(target_logp_est), xmin=0, xmax=epochs[-1], color='green', linestyle='--', label='exact', linewidth=2\n",
    "    # )\n",
    "    axes[3].set_title(r\"$\\log p$ from PFlow and SDE\")\n",
    "    axes[3].legend(loc='best')\n",
    "    axes[3].set_ylim(-7,0)\n",
    "\n",
    "\n",
    "    fig.suptitle(r\"$\\epsilon = $\" + str(grab(eps)) + r\" $n_{likelihood} = $\" + str(n_likelihood), fontsize=16, y = 1.05)\n",
    "    plt.savefig(\"figs/training-checker.pdf\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def train_step(\n",
    "    prior_bs: int,\n",
    "    target_bs: int,\n",
    "    N_t: int,\n",
    "    interpolant: stochastic_interpolant.Interpolant,\n",
    "    opt: Any,\n",
    "    sched: Any\n",
    "):\n",
    "    \"\"\"\n",
    "    Take a single step of optimization on the training set.\n",
    "    \"\"\"\n",
    "    opt.zero_grad()\n",
    "\n",
    "\n",
    "    # construct batch\n",
    "    x0s = base(prior_bs)\n",
    "    x1s = target(target_bs)\n",
    "    ts  = torch.rand(size=(N_t,))\n",
    "\n",
    "\n",
    "    # compute the loss\n",
    "    loss_start = time.perf_counter()\n",
    "    loss_val, (loss_v, loss_s) = stochastic_interpolant.loss_sv(\n",
    "        v, s, x0s, x1s, ts, interpolant, loss_fac=loss_fac\n",
    "    )\n",
    "    loss_end = time.perf_counter()\n",
    "\n",
    "\n",
    "    # compute the gradient\n",
    "    backprop_start = time.perf_counter()\n",
    "    loss_val.backward()\n",
    "    v_grad = torch.tensor([torch.nn.utils.clip_grad_norm_(v.parameters(), float('inf'))])\n",
    "    s_grad = torch.tensor([torch.nn.utils.clip_grad_norm_(s.parameters(), float('inf'))])\n",
    "    backprop_end = time.perf_counter()\n",
    "\n",
    "\n",
    "    # perform the update.\n",
    "    update_start = time.perf_counter()\n",
    "    opt.step()\n",
    "    sched.step()\n",
    "    update_end = time.perf_counter()\n",
    "\n",
    "\n",
    "    if counter < 5:\n",
    "        print(f'[Loss: {loss_end - loss_start}], [Backprop: {backprop_end-backprop_start}], [Update: {update_end-update_start}].')\n",
    "\n",
    "\n",
    "    return loss_val.detach(), loss_v.detach(), loss_s.detach(), v_grad.detach(), s_grad.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2670e713-e83c-4ec7-84f3-3dd4f0912f2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af7ce2eb",
   "metadata": {},
   "source": [
    "### Define target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ff28bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ndim = 2\n",
    "def target(bs):\n",
    "    x1 = torch.rand(bs) * 4 - 2\n",
    "    x2_ = torch.rand(bs) - torch.randint(2, (bs,)) * 2\n",
    "    x2 = x2_ + (torch.floor(x1) % 2)\n",
    "    return (torch.cat([x1[:, None], x2[:, None]], 1) * 2)\n",
    "\n",
    "\n",
    "target_samples = grab(target(10000))\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "plt.hist2d(target_samples[:,0], target_samples[:,1], bins = 100, range=[[-4,4],[-4,4]]);\n",
    "plt.title(\"Checker Target\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"Batch Shape:\", target_samples.shape)\n",
    "# target_logp_est = target.log_prob(target(10000)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a47a52",
   "metadata": {},
   "source": [
    "### Define Base Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27190b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_loc     = torch.zeros(ndim)\n",
    "base_var     = torch.ones(ndim)\n",
    "base         = prior.SimpleNormal(base_loc, 1.0*base_var)\n",
    "base_samples = grab(base(20000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90eccf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6,))\n",
    "plt.scatter(base_samples[:,0], base_samples[:,1],  label = 'base', alpha = 0.2);\n",
    "plt.scatter(target_samples[:,0], target_samples[:,1], alpha = 0.2);\n",
    "plt.title(\"Bimodal Target\")\n",
    "plt.title(\"Base vs Target\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e4caad",
   "metadata": {},
   "source": [
    "### Define Interpolant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb13153",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = lambda t: t*(1-t)\n",
    "gamma_dot = lambda t: 1 -2*t\n",
    "gg_dot = lambda t: gamma(t)*gamma_dot(t)\n",
    "\n",
    "# gamma = lambda t: torch.sqrt(t*(1-t))\n",
    "# gamma_dot = lambda t: (1/(2*torch.sqrt(t*(1-t)))) * (1 -2*t)\n",
    "# gg_dot = lambda t: (1/2)*(1-2*t)\n",
    "\n",
    "# gamma = lambda t: torch.sin(math.pi * t)**2\n",
    "# gamma_dot = lambda t: 2*math.pi*torch.sin(math.pi * t)*torch.cos(math.pi*t)\n",
    "\n",
    "interpolant  = stochastic_interpolant.Interpolant(path='linear', \n",
    "                                                          gamma=gamma, \n",
    "                                                          gamma_dot=gamma_dot,\n",
    "                                                          gg_dot = gg_dot,\n",
    "                                                          It=None, dtIt=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df9c81d",
   "metadata": {},
   "source": [
    "### Define velocity field and optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba805f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_lr      = 2e-3\n",
    "hidden_sizes = [150, 150, 150, 150]\n",
    "in_size      = (ndim+1)\n",
    "out_size     = (ndim)\n",
    "inner_act    = 'relu'\n",
    "final_act    = 'none'\n",
    "print_model  = False\n",
    "\n",
    "\n",
    "v     = itf.fabrics.make_fc_net(hidden_sizes=hidden_sizes, in_size=in_size, out_size=out_size, inner_act=inner_act, final_act=final_act)\n",
    "s     = itf.fabrics.make_fc_net(hidden_sizes=hidden_sizes, in_size=in_size, out_size=out_size, inner_act=inner_act, final_act=final_act)\n",
    "opt   = torch.optim.Adam([*v.parameters(), *s.parameters()], lr=base_lr)\n",
    "sched = torch.optim.lr_scheduler.StepLR(optimizer=opt, step_size=1500, gamma=0.4)\n",
    "\n",
    "\n",
    "eps          = torch.tensor(0.5)\n",
    "N_era        = 14\n",
    "N_epoch      = 500\n",
    "N_t          = 50    # number of time steps in batch (e.g. to make samples from rho_t)\n",
    "plot_bs      = 5000 # number of samples to use when plotting\n",
    "prior_bs     = 25   # number of samples from rho_0 in batch\n",
    "target_bs    = 100   # number of samples from rho_1 in batch\n",
    "metrics_freq = 50   # how often to log metrics, e.g. if logp is not super cheap don't do it everytime\n",
    "plot_freq    = 500  # how often to plot\n",
    "n_save       = 10   # how often to checkpoint SDE integrator\n",
    "loss_fac     = 4.0 # ratio of learning rates for w to v\n",
    "n_likelihood = 20    # number of trajectories used to compute the SDE likelihood\n",
    "\n",
    "\n",
    "if print_model:\n",
    "    print(\"Here's the model v, s:\", v, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f33113",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\n",
    "    'losses': [],\n",
    "    'v_losses': [],\n",
    "    's_losses': [],\n",
    "    'v_grads': [],\n",
    "    's_grads': [],\n",
    "    'times': [],\n",
    "    'logps_pflow': [],\n",
    "    'logps_sdeflow': []\n",
    "}\n",
    "\n",
    "counter = 1\n",
    "for i, era in enumerate(range(N_era)):\n",
    "    for j, epoch in enumerate(range(N_epoch)):\n",
    "        loss, v_loss, s_loss, v_grad, s_grad = train_step(\n",
    "            prior_bs, target_bs, N_t, interpolant, opt, sched\n",
    "        )\n",
    "\n",
    "\n",
    "        if (counter - 1) % metrics_freq == 0:\n",
    "            log_metrics(v, s, interpolant, n_save, n_likelihood, prior_bs, v_loss, \n",
    "                        s_loss, loss, v_grad, s_grad, eps, data_dict)\n",
    "\n",
    "\n",
    "        if (counter - 1) % plot_freq == 0:\n",
    "            make_plots(v, s, interpolant, n_save, n_likelihood, plot_bs, counter, metrics_freq, eps, data_dict)\n",
    "\n",
    "\n",
    "        counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdff791-3022-4e52-b4f5-d9c7199a2daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps          = torch.tensor(0.5)\n",
    "\n",
    "\n",
    "make_plots(v, s, interpolant, n_save, 10*n_likelihood, plot_bs, counter, metrics_freq, eps, data_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0664a840-d61b-44f6-b23f-e03d5dfe37d6",
   "metadata": {},
   "source": [
    "### Save models for plotting later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef15854e-0ece-416a-a569-9b199fe8f92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logdir = \"/mnt/home/malbergo/InterpolantFlow/notebooks/figs/models_for_figs/\"\n",
    "\n",
    "# model_fname_sd = os.path.join(logdir, f'checker_state_dict_v-2.pt')\n",
    "# model_fname = os.path.join(logdir, f'checker_v-2.pt')\n",
    "# torch.save(v.state_dict(), model_fname_sd)\n",
    "# torch.save(v, model_fname)\n",
    "\n",
    "# model_fname_sd = os.path.join(logdir, f'checker_state_dict_s-2.pt')\n",
    "# model_fname = os.path.join(logdir, f'checker_s-2.pt')\n",
    "# torch.save(s.state_dict(), model_fname_sd)\n",
    "# torch.save(s, model_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31120f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(111)\n",
    "\n",
    "with torch.no_grad():\n",
    "    sde_flow  = itf.stochastic_interpolant.SDEIntegrator(v=v, s=s, dt=torch.tensor(1e-2), eps=eps, n_save=n_save)\n",
    "    bs  = 75000\n",
    "    x0s = base(bs)\n",
    "    xfs = sde_flow.rollout_forward(x0s)\n",
    "    xf = grab(xfs[-1].squeeze())\n",
    "\n",
    "    fig, ax = plt.subplots(1,1,figsize=(5,5))\n",
    "    ax.scatter(xf[:,0], xf[:,1], vmin=0.0, vmax=0.05, alpha = 0.01)\n",
    "    ax.set_xlim(-5,5)\n",
    "    ax.set_ylim(-6,6)\n",
    "    ax.set_title(\"Samples from v,w SDE Model, bs = \" + str(bs) + \", eps = \" + str(grab(eps)), fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef60ee5-cfe5-4433-a07b-ccd8740751d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(111)\n",
    "\n",
    "with torch.no_grad():\n",
    "    pflow = itf.stochastic_interpolant.PFlowIntegrator(v=v, s=s, method='dopri5', eps=interpolant.eps, n_step=3)\n",
    "    bs  = 75000\n",
    "    x0s = base(bs)\n",
    "    xfs, _ = pflow.rollout(x0s)\n",
    "    print(xfs.shape)\n",
    "    xf = grab(xfs[-1].squeeze())\n",
    "    print(xf.shape)\n",
    "\n",
    "    fig, ax = plt.subplots(1,1,figsize=(5,5))\n",
    "    ax.scatter(xf[:,0], xf[:,1], vmin=0.0, vmax=0.05, alpha = 0.01)\n",
    "    ax.set_xlim(-5,5)\n",
    "    ax.set_ylim(-6,6)\n",
    "    ax.set_title(\"Samples from v,w ODE Model, bs = \" + str(bs) + \", eps = \" + str(grab(eps)), fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18270bc",
   "metadata": {},
   "source": [
    "### Make a plot of xt over time from integrating $v_t(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889c78c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip = 2\n",
    "\n",
    "ncol = len(xfs) // skip\n",
    "fig, axes = plt.subplots(1, ncol, figsize=(ncol*4,4))\n",
    "bins = 30\n",
    "\n",
    "ts = np.linspace(0,1,10)\n",
    "\n",
    "for i in range(len(xfs)):\n",
    "    if i == len(xf) - 1:\n",
    "        time_slice = grab(xfs[-1])\n",
    "        axes[-1].scatter(time_slice[:,0], time_slice[:,1], label = 'diffused from $N(0, I_2)$', alpha = 0.02)\n",
    "        axes[-1].scatter(target_samples[:,0], target_samples[:,1], alpha = 0.02,  label= ' true target mixture')\n",
    "        axes[-1].set_xticks([])\n",
    "        axes[-1].set_title('$t = %.2f$' % ((ts[i])))\n",
    "    elif (i-1) % skip == 0:\n",
    "        ind = (i-1) // skip\n",
    "        \n",
    "        time_slice = grab(xfs[i])\n",
    "        axes[ind].scatter(time_slice[:,0], time_slice[:,1], label = 'diffused from $N(0, I_2)$', alpha = 0.02)\n",
    "        axes[ind].scatter(target_samples[:,0], target_samples[:,1], alpha = 0.02,  label= ' true target mixture')\n",
    "        axes[ind].set_xticks([])\n",
    "        axes[ind].set_title('$t = %.2f$' % ((ts[i])))\n",
    "        if (i-1) !=0:\n",
    "            axes[ind].set_yticks([])\n",
    "        else:\n",
    "            axes[ind].set_ylabel(\"histogram density\", fontsize=16)\n",
    "fig.text(x=0.5, y = 0.0, s=\"x\", fontsize = 16)\n",
    "fig.text(0.91, 0.5, '$\\cdot$ true', bbox={'facecolor': 'lightblue', 'pad': 4})\n",
    "fig.text(0.91, 0.4, '$\\cdot$ model', bbox={'facecolor': 'xkcd:orange', 'pad': 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3af35e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "afm",
   "language": "python",
   "name": "afm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
