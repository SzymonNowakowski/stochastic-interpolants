{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55eb47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchdiffeq import odeint_adjoint as odeint\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from typing import Tuple, Any\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import interflow as itf\n",
    "import interflow.prior as prior\n",
    "import interflow.fabrics\n",
    "import interflow.stochastic_interpolant as stochastic_interpolant\n",
    "from torch import autograd\n",
    "from functorch import jacfwd, vmap\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('CUDA available, setting default tensor residence to GPU.')\n",
    "    itf.util.set_torch_device('cuda')\n",
    "else:\n",
    "    print('No CUDA device found!')\n",
    "print(itf.util.get_torch_device())\n",
    "\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f48119",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3330e609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab(var):\n",
    "    \"\"\"Take a tensor off the gpu and convert it to a numpy array on the CPU.\"\"\"\n",
    "    return var.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "def compute_likelihoods(\n",
    "    s: torch.nn.Module,\n",
    "    interpolant: stochastic_interpolant.Interpolant,\n",
    "    n_save: int,\n",
    "    n_step: int,\n",
    "    eps: int,\n",
    "    bs: int\n",
    ") -> Tuple[torch.tensor, torch.tensor]:\n",
    "    \"\"\"Draw samples from the probability flow and SDE models, and compute likelihoods.\"\"\"\n",
    "    \n",
    "    \n",
    "    sde_flow = stochastic_interpolant.MirrorSDEIntegrator(\n",
    "        s=s, eps=eps, interpolant=interpolant, n_save=n_save, n_likelihood=1, n_step=n_step\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        x0_tests  = target(bs)\n",
    "        xfs_sde   = sde_flow.rollout_forward(x0_tests) # [n_save x bs x dim]\n",
    "        xf_sde    = grab(xfs_sde[-1].squeeze())        # [bs x dim]\n",
    "        x0s_sdeflow, _ = sde_flow.rollout_likelihood(xfs_sde[-1])\n",
    "    \n",
    "    return xf_sde\n",
    "\n",
    "\n",
    "def log_metrics(\n",
    "    s: torch.nn.Module,\n",
    "    interpolant: stochastic_interpolant.Interpolant,\n",
    "    n_save: int,\n",
    "    n_step: int,\n",
    "    likelihood_bs: int, \n",
    "    s_loss: torch.tensor,\n",
    "    s_grad: torch.tensor,\n",
    "    eps: torch.tensor,\n",
    "    data_dict: dict\n",
    ") -> None:\n",
    "    # log loss and gradient data\n",
    "    s_loss   = grab(s_loss).mean(); data_dict['s_losses'].append(s_loss)\n",
    "    s_grad   = grab(s_grad).mean(); data_dict['s_grads'].append(s_grad)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "def make_plots(\n",
    "    s: torch.nn.Module,\n",
    "    interpolant: stochastic_interpolant.Interpolant,\n",
    "    n_save: int,\n",
    "    n_step: int, ## number of sde steps in [0,1]\n",
    "    likelihood_bs: int,\n",
    "    counter: int,\n",
    "    metrics_freq: int,\n",
    "    eps: torch.tensor,\n",
    "    data_dict: dict\n",
    ") -> None:\n",
    "    \"\"\"Make plots to visualize samples and evolution of the likelihood.\"\"\"\n",
    "    # compute likelihood and samples for SDE and probability flow.\n",
    "    xf_sde = compute_likelihoods(\n",
    "        s, interpolant, n_save, n_step, eps, likelihood_bs\n",
    "    )\n",
    "\n",
    "\n",
    "    ### plot the loss, test logp, and samples from interpolant flow\n",
    "    fig, axes = plt.subplots(1,2, figsize=(12,4))\n",
    "    print(\"EPOCH:\", counter)\n",
    "    print(\"LOSS, GRAD:\", s_loss, s_grad)\n",
    "\n",
    "\n",
    "    # plot loss over time.\n",
    "    nsaves = len(data_dict['s_losses'])\n",
    "    epochs = np.arange(nsaves)*metrics_freq\n",
    "    axes[0].plot(epochs, data_dict['s_losses'], label = \"s\" )\n",
    "    axes[0].set_title(\"LOSS\")\n",
    "    axes[0].legend()\n",
    "\n",
    "\n",
    "    # plot samples from SDE.\n",
    "    axes[1].scatter(\n",
    "        xf_sde[:,0], xf_sde[:,1], vmin=0.0, vmax=0.05, alpha = 0.2)\n",
    "    axes[1].set_xlim(-5,5)\n",
    "    axes[1].set_ylim(-6.5,6.5)\n",
    "    axes[1].set_title(\"Samples from SDE\", fontsize=14)\n",
    "\n",
    "\n",
    "\n",
    "    fig.suptitle(r\"$\\epsilon = $\" + str(grab(eps)),  fontsize=16, y = 1.05)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def train_step(\n",
    "    bs: int,\n",
    "    interpolant: stochastic_interpolant.Interpolant,\n",
    "    opt_s  : Any,\n",
    "    sched_s: Any,\n",
    "    lower_upper: tuple = (0.0001, 0.9999)\n",
    "):\n",
    "    \"\"\"\n",
    "    Take a single step of optimization on the training set.\n",
    "    \"\"\"\n",
    "    opt_s.zero_grad()\n",
    "    lower, upper = lower_upper[0], lower_upper[1]\n",
    "\n",
    "    # construct batch\n",
    "    \n",
    "    x0s = base(bs)\n",
    "    x1s = target(bs)\n",
    "    ts  = lower + (upper - lower)*torch.rand(size=(bs,))\n",
    "\n",
    "\n",
    "    # compute the loss\n",
    "    loss_start = time.perf_counter()\n",
    "    loss_s     = loss_fn_s(s, x0s, x1s, ts, interpolant)\n",
    "    loss_end   = time.perf_counter()\n",
    "\n",
    "\n",
    "    # compute the gradient\n",
    "    backprop_start = time.perf_counter()\n",
    "    loss_s.backward()\n",
    "    s_grad = torch.tensor([torch.nn.utils.clip_grad_norm_(s.parameters(), float('inf'))])\n",
    "    backprop_end = time.perf_counter()\n",
    "\n",
    "\n",
    "    # perform the update.\n",
    "    update_start = time.perf_counter()\n",
    "    opt_s.step()\n",
    "    sched_s.step()\n",
    "    update_end = time.perf_counter()\n",
    "\n",
    "\n",
    "    if counter < 5:\n",
    "        print(f'Timing [Loss: {loss_end - loss_start}], [Backprop: {backprop_end-backprop_start}], [Update: {update_end-update_start}].')\n",
    "\n",
    "\n",
    "    return loss_s.detach(), s_grad.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7ce2eb",
   "metadata": {},
   "source": [
    "### Define target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ff28bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ndim = 2\n",
    "def target(bs):\n",
    "    x1 = torch.rand(bs) * 4 - 2\n",
    "    x2_ = torch.rand(bs) - torch.randint(2, (bs,)) * 2\n",
    "    x2 = x2_ + (torch.floor(x1) % 2)\n",
    "    return (torch.cat([x1[:, None], x2[:, None]], 1) * 2)\n",
    "\n",
    "\n",
    "target_samples = grab(target(10000))\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "plt.hist2d(target_samples[:,0], target_samples[:,1], bins = 100, range=[[-4,4],[-4,4]]);\n",
    "plt.title(\"Checker Target\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"Batch Shape:\", target_samples.shape)\n",
    "# target_logp_est = target.log_prob(target(10000)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a47a52",
   "metadata": {},
   "source": [
    "### Define Base Distribution which functions as the intermediate noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed40e443-b5a3-40cb-a3a7-0e801b563120",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_loc     = torch.zeros(ndim)\n",
    "base_var     = torch.ones(ndim)\n",
    "base         = prior.SimpleNormal(base_loc, 1.0*base_var)\n",
    "base_samples = grab(base(20000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90eccf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6,))\n",
    "plt.scatter(base_samples[:,0], base_samples[:,1],  label = 'base', alpha = 0.2);\n",
    "plt.scatter(target_samples[:,0], target_samples[:,1], alpha = 0.2);\n",
    "plt.title(\"Bimodal Target\")\n",
    "plt.title(\"Base vs Target\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e4caad",
   "metadata": {},
   "source": [
    "### Define Interpolant: A mirror interpolant, meaning that $x_0 \\sim \\rho_{\\text{data}}$ $x_1 \\sim N(0,1)$ and $x_t = x_0 + \\gamma(t)x_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3bb13153",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_type = 'brownian'\n",
    "path = 'mirror'\n",
    "interpolant  = stochastic_interpolant.Interpolant(path=path, gamma_type=gamma_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c00b60-2f61-4b6f-8a93-3a2099ed6535",
   "metadata": {},
   "source": [
    "### Define losses for b and s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb6afef-c2d2-4f96-979f-642cff8a7922",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn_s = stochastic_interpolant.make_loss(method='shared', \n",
    "                                             interpolant = interpolant, loss_type='mirror')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df9c81d",
   "metadata": {},
   "source": [
    "### Define velocity field and optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba805f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_lr      = 2e-3\n",
    "hidden_sizes = [256, 256, 256, 256]\n",
    "in_size      = (ndim+1)\n",
    "out_size     = (ndim)\n",
    "inner_act    = 'relu'\n",
    "final_act    = 'none'\n",
    "print_model  = False\n",
    "\n",
    "\n",
    "s       = itf.fabrics.make_fc_net(hidden_sizes=hidden_sizes, in_size=in_size, out_size=out_size, inner_act=inner_act, final_act=final_act)\n",
    "opt_s   = torch.optim.Adam(s.parameters(), lr=base_lr)\n",
    "sched_s = torch.optim.lr_scheduler.StepLR(optimizer=opt_s, step_size=1500, gamma=0.4)\n",
    "\n",
    "\n",
    "eps          = torch.tensor(0.5)\n",
    "N_era        = 14\n",
    "N_epoch      = 500\n",
    "plot_bs      = 5000  # number of samples to use when plotting\n",
    "bs           = 2000    # number of samples from rho_0 in batch\n",
    "metrics_freq = 50    # how often to log metrics, e.g. if logp is not super cheap don't do it everytime\n",
    "plot_freq    = 500   # how often to plot\n",
    "n_save       = 10    # how often to checkpoint SDE integrator\n",
    "loss_fac     = 4.0   # ratio of learning rates for w to v\n",
    "n_step       = 100   # number of steps taken by the SDE in [0,1]\n",
    "\n",
    "\n",
    "if print_model:\n",
    "    print(\"Here's the model s:\", s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f33113",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\n",
    "    's_losses': [],\n",
    "    's_grads': [],\n",
    "    'times': [],\n",
    "}\n",
    "\n",
    "counter = 1\n",
    "for i, era in enumerate(range(N_era)):\n",
    "    for j, epoch in enumerate(range(N_epoch)):\n",
    "        s_loss, s_grad = train_step(bs, interpolant, opt_s, sched_s,\n",
    "        )\n",
    "\n",
    "\n",
    "        if (counter - 1) % metrics_freq == 0:\n",
    "            log_metrics(s, interpolant, n_save, n_step, bs, \n",
    "                        s_loss, s_grad, eps, data_dict)\n",
    "\n",
    "\n",
    "        if (counter - 1) % plot_freq == 0:\n",
    "            make_plots(s, interpolant, n_save, n_step, plot_bs, counter, metrics_freq, eps, data_dict)\n",
    "\n",
    "\n",
    "        counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58ad807-f71b-4d39-8f13-85fedd706631",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_step = 600\n",
    "bs = 40000\n",
    "n_save = n_step // 4\n",
    "eps = torch.tensor(6.0)\n",
    "sde_flow = stochastic_interpolant.MirrorSDEIntegrator(\n",
    "    s=s, \n",
    "    start_end=(lower, upper),\n",
    "    n_step=n_step,\n",
    "    eps=eps, \n",
    "    interpolant=interpolant,\n",
    "    n_save=n_save, \n",
    "    n_likelihood=1\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    # x0_tests  = target(bs)\n",
    "    x0_tests  = torch.ones(size=(bs, 2))\n",
    "    xfs_sde   = sde_flow.rollout_forward(x0_tests) # [n_save x bs x dim]\n",
    "    xf_sde    = grab(xfs_sde[-1].squeeze())        # [bs x dim]\n",
    "\n",
    "xf_true = grab(target(40000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af00cf1b-2718-46dd-9a84-3c62038cdea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_sde = grab(x0_tests)\n",
    "plt.scatter(xf_sde[:,0], xf_sde[:,1], alpha = 0.1, label=r'samples from evolving $x-0$')\n",
    "plt.scatter(init_sde[:,0], init_sde[:,1], alpha=0.5, label=r'initial condition $x_0$')\n",
    "plt.title(\"Full distribution generated from a single initial condition!\")\n",
    "plt.legend(loc = 'upper left', bbox_to_anchor=(1.0, 0.6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea3addd-0ef0-425a-880c-5d9468996ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "afm",
   "language": "python",
   "name": "afm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
